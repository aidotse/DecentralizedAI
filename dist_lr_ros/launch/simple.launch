<?xml version="1.0"?>
<launch>

  <!-- Global parameters -->
  <arg name="model"            default="mlp"   />
  <arg name="dp"               default="false" doc="Use of diffirentla privacy (or not)." />
  <arg name="learning_rate"    default="0.01"  />
  <arg name="batch_size"       default="32"    />
  <arg name="epochs"           default="10"    doc="Training epochs for each communcation round." />

  <!-- Dataset loader -->	
  <node name="data_loader_node" pkg="data_loader" type="dataset.py" output="screen" />

  <!-- Dstributed worker(s) -->
  <!-- Note: name must be unique for each worker -->

  <!-- Worker A -->
  <node name="worker_a_node" pkg="dist_lr_ros" type="worker.py" output="screen">
    <param name="model"           type="str"    value="$(arg model)" />
    <param name="dp"              type="bool"   value="$(arg dp)" />
    <param name="learning_rate"   type="double" value="$(arg learning_rate)" />
    <param name="batch_size"      type="int"    value="$(arg batch_size)" />
    <param name="epochs"          type="int"    value="$(arg epochs)" />
    <param name="dataset_portion" type="double" value="0.1" />
  </node>

  <!-- Worker B -->
  <node name="worker_b_node" pkg="dist_lr_ros" type="worker.py" output="screen">
    <param name="model"           type="str"    value="$(arg model)" />
    <param name="dp"              type="bool"   value="$(arg dp)" />
    <param name="learning_rate"   type="double" value="$(arg learning_rate)" />
    <param name="batch_size"      type="int"    value="$(arg batch_size)" />
    <param name="epochs"          type="int"    value="$(arg epochs)" />
    <param name="dataset_portion" type="double" value="0.1" />
  </node>

</launch>
